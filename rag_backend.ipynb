{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2556c63-c2d0-490e-b1d0-a5ba740951de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting rag_backend.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile rag_backend.py\n",
    "import re, os, tempfile, pickle\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# ---------- PDF ----------\n",
    "def pdf_to_text(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    pages = []\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            pages.append(text)\n",
    "    return pages\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# ---------- Text chunking ----------\n",
    "def chunk_text(text, chunk_size=800, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    L = len(text)\n",
    "    while start < L:\n",
    "        end = min(L, start + chunk_size)\n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = max(end - overlap, end)\n",
    "    return [c for c in chunks if c.strip()]\n",
    "\n",
    "def make_safe_chunks(pages, tokenizer, chunk_size_chars=800, overlap_chars=200, max_tokens=500):\n",
    "    raw = []\n",
    "    for p in pages:\n",
    "        raw.extend(chunk_text(p, chunk_size_chars, overlap_chars))\n",
    "    safe = []\n",
    "    for r in raw:\n",
    "        tokens = tokenizer.tokenize(r)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            safe.append(r)\n",
    "        else:\n",
    "            for i in range(0, len(tokens), max_tokens):\n",
    "                sub = tokens[i:i+max_tokens]\n",
    "                safe.append(tokenizer.convert_tokens_to_string(sub))\n",
    "    return safe\n",
    "\n",
    "# ---------- FAISS ----------\n",
    "class FaissStore:\n",
    "    def __init__(self, dim):\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.metadatas = []\n",
    "\n",
    "    def add(self, vectors, metadatas):\n",
    "        self.index.add(vectors.astype('float32'))\n",
    "        self.metadatas.extend(metadatas)\n",
    "\n",
    "    def search(self, q_vec, top_k=4):\n",
    "        q = q_vec.astype('float32')\n",
    "        if q.ndim == 1:\n",
    "            q = q.reshape(1, -1)\n",
    "        D, I = self.index.search(q, top_k)\n",
    "        results = []\n",
    "        for j, idx in enumerate(I[0]):\n",
    "            if idx < len(self.metadatas):\n",
    "                results.append((self.metadatas[idx], float(D[0][j])))\n",
    "        return results\n",
    "\n",
    "# ---------- Prompt ----------\n",
    "def build_prompt(question, retrieved_texts, max_context_chars=3000):\n",
    "    context = \"\\n\\n\".join(retrieved_texts)\n",
    "    if len(context) > max_context_chars:\n",
    "        context = context[:max_context_chars]\n",
    "    return f\"Answer the question using ONLY the context below:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "# ---------- RAG pipeline ----------\n",
    "def build_rag_system(embed_model_name, gen_model_name, pdf_path, chunk_size=800, overlap=200):\n",
    "    embed_model = SentenceTransformer(embed_model_name)\n",
    "    gen_model = pipeline(\"text2text-generation\", model=gen_model_name, device=-1)\n",
    "\n",
    "    pages = pdf_to_text(pdf_path)\n",
    "    pages = [clean_text(p) for p in pages if p.strip()]\n",
    "\n",
    "    list_of_chunks = make_safe_chunks(pages, embed_model.tokenizer, chunk_size, overlap)\n",
    "    vectors = embed_model.encode(list_of_chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "    vectors = normalize(vectors, axis=1)\n",
    "\n",
    "    fa = FaissStore(vectors.shape[1])\n",
    "    metas = [{\"text\": c, \"source\": os.path.basename(pdf_path)} for c in list_of_chunks]\n",
    "    fa.add(vectors, metas)\n",
    "\n",
    "    return embed_model, gen_model, fa, list_of_chunks\n",
    "\n",
    "def ask_question(question, embed_model, gen_model, fa_store, top_k=3):\n",
    "    q_vec = embed_model.encode([question], convert_to_numpy=True)\n",
    "    q_vec = normalize(q_vec, axis=1)\n",
    "    retrieved = fa_store.search(q_vec, top_k=top_k)\n",
    "    retrieved_texts = [r[0][\"text\"] for r in retrieved]\n",
    "    prompt = build_prompt(question, retrieved_texts)\n",
    "    out = gen_model(prompt, max_length=256, do_sample=False)\n",
    "    answer = out[0][\"generated_text\"]\n",
    "    return answer, retrieved_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb2e7e-afac-4c14-81f0-e11a72fa0e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
